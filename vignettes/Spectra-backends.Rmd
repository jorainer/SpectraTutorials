---
title: "Spectra: an expandable infrastructure to handle mass spectrometry data"
author: "Johannes Rainer^[Institute for Biomedicine, Eurac Research, Bolzano,
Italy; johannes.rainer@eurac.edu], Sebastian Gibb^[Department of Anaesthesiology
and Intensive Care, University Medicine Greifswald, Germany], Laurent
Gatto^[Computational Biology Unit, de Duve Institute, UCLouvain, Brussels, Belgium]"
output:
  rmarkdown::html_document:
    highlight: pygments
    toc: true
    toc_depth: 3
    fig_width: 5
vignette: >
  %\VignetteIndexEntry{Spectra: an expandable infrastructure to handle mass spectrometry data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding[utf8]{inputenc}
  %\VignettePackage{SpectraTutorials}
  %\VignetteDepends{Spectra,mzR,BiocStyle,msdata,MsBackendSql,RSQLite,microbenchmark}
bibliography: references.bib
---

```{r style, echo = FALSE, results = 'asis', message = FALSE}
library(BiocStyle)
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

**Last modified:** `r file.info("Spectra-backends.Rmd")$mtime`<br />
**Compiled**: `r date()`


# Abstract

Mass spectrometry (MS) data is a key technology in modern metabolomics and
proteomics experiments. Continuous improvements in MS instrumentation, larger
experiments and new technological developments lead to ever growing data sizes
and increased number of available variables making standard in-memory data
handling and processing difficult.

The `r BiocStyle::Biocpkg("Spectra")` package provides a modern infrastructure
for MS data handling specifically designed to enable extension to additional
data resources or alternative data representations. These can be realized by
extending the virtual `MsBackend` class and its related methods. Implementations
of such `MsBackend` classes can be tailored for specific needs, such as low
memory footprint, fast processing, remote data access, or also support for
specific additional data types or variables. Importantly, data processing of
`Spectra` objects is independent of the backend in use due to a lazy evaluation
mechanism that caches data manipulations internally.

This workshop discusses different available data representations for MS data
along with their properties, advantages and performances. In addition,
*Spectra*'s concept of lazy evaluation for data manipulations is presented, as
well as a simple caching mechanism for data modifications. Finally, it explains
how new `MsBackend` instances can be implemented and tested to ensure
compliance.


# Introduction

This workshop/tutorial assumes that readers are familiar with mass spectrometry
data. See the [*LC-MS/MS in a
nutshell*](https://jorainer.github.io/SpectraTutorials/articles/analyzing-MS-data-from-different-sources-with-Spectra.html#lc-msms-in-a-nutshell)
section of the *Seamless Integration of Mass Spectrometry Data from Different
Sources* vignette in this package for a general introduction to MS.


## Pre-requisites

- Basic familiarity with R and Bioconductor.
- Basic understanding of Mass Spectrometry (MS) data.


## Installation

- Get the [docker image](https://hub.docker.com/r/jorainer/spectra_tutorials) of
  this tutorial with `docker pull jorainer/spectra_tutorials:latest`.
- Start docker using
  ```
  docker run \
      -e PASSWORD=bioc \
      -p 8787:8787 \
      jorainer/spectra_tutorials:latest
  ```
- Enter `http://localhost:8787` in a web browser and log in with username
  `rstudio` and password `bioc`.
- Open this R-markdown file (*vignettes/Spectra-backends.Rmd*) in the RStudio
  server version in the web browser and evaluate the R code blocks.


## R/Bioconductor packages used

- [`Spectra`](https://bioconductor.org/packages/Spectra)
- [`MsBackendSql`](https://bioconductor.org/packages/MsBackendSql)


## Workshop goals and objectives

This is a technical demonstration of the internals of the *Spectra* package and
the design of its MS infrastructure. We're not demonstrating any use cases or
analysis workflows here.


### Learning goals

- Understand how MS data is handled with *Spectra*.
- Understand differences and properties of different `MsBackend`
  implementations.


### Learning objectives

- Learn how MS data is handled with *Spectra*.
- Understand which data representations/backends fit which purpose/use case.
- Insights into the internals of the *Spectra* MS infrastructure to facilitate
  implementation of own backend(s).


# Workshop

## The *Spectra* package

- **Purpose**: provide an expandable, well tested and user-friendly
  infrastructure for mass spectrometry (MS) data.
- **Design**: separation of code representing the main user interface and code
  to provide, store and read MS data.

![Spectra: separation into user functionality and data
representation](Spectra.png)

- `Spectra`: main interface for the end user.
- `MsBackend`: defines how and where data is stored and how it is managed.
- **Why?**:
  - the user does not need to care about where or how the data is stored.
  - the same functionality can be applied to MS data, regardless of where and
    how the data is stored.
  - enables specialized data storage/representation options: remote data, low
    memory footprint, high performance.


## Creating and using `Spectra` objects

MS data consists of duplets of mass-to-charge (*m/z*) and intensity values along
with potential additional information, such as the MS level, retention time,
polarity etc. In its simplest form, MS data consists thus of two (aligned)
numeric vectors with the *m/z* and intensity values and an e.g. `data.frame`
containing potential additional annotations for a MS spectrum. In `Spectra`
terminology, `"mz"` and `"intensity"` are called *peak variables* (because they
provide information on individual mass peaks), and all other annotations
*spectra variables* (usually being a single value per spectrum).

Below we define *m/z* and intensity values for a mass spectrum as well as a
`data.frame` with additional *spectra variables*.

```{r}
#' Define simple MS data
mz <- c(56.0494, 69.0447, 83.0603, 109.0395, 110.0712,
        111.0551, 123.0429, 138.0662, 195.0876)
int <- c(0.459, 2.585, 2.446, 0.508, 8.968, 0.524, 0.974, 100.0, 40.994)

sv <- data.frame(msLevel = 1L, polarity = 1L)
```

This would be a basic representation of MS data in R. For obvious reasons it is
however better to store such data in a single *container* than keeping it in
separate, unrelated, variables. We thus create below a `Spectra` object from
this MS data. `Spectra` objects can be created with the `Spectra` constructor
function that accepts (among other possibilities) a `data.frame` with the full
MS data as input. Each row in that `data.frame` is expected to contain data from
one spectrum. We therefore need to add the *m/z* and intensity values as a
`list` of numerical vectors to our data frame.

```{r}
library(Spectra)

#' wrap m/z and intensities into a list and add them to the data.frame
sv$mz <- list(mz)
sv$intensity <- list(int)

#' Create a `Spectra` object from the data.
s <- Spectra(sv)
s
```

We have now created a `Spectra` object representing our toy MS data. Individual
spectra variables can be accessed with either `$` and the name of the spectra
variable, or with one of the dedicated accessor functions.

```{r}
#' Access the MS level spectra variable
s$msLevel
msLevel(s)
```

Also, while we provided only the spectra variables `"msLevel"` and `"polarity"`
with out input data frame, more variables are available by default for a
`Spectra` object. These are called *core spectra variables* and they can
**always** be extracted from a `Spectra` object, even if they are not defined
(in which case missing values are returned). Spectra variables available from a
`Spectra` object can be listed with the `spectraVariables` function:

```{r}
#' List all available spectra variables
spectraVariables(s)

#' Extract the retention time spectra variable
s$rtime
```

We've now got a `Spectra` object representing a single MS spectrum - but, as the
name of the class implies, it is actually designed to represent data from
multiple mass spectra (possibly from a whole experiment). Below we thus define
again a `data.frame`, this time with 3 rows, and create a `Spectra` object from
that. Also, we define some additional spectra variables providing the name and
ID of the compounds the MS2 (fragment) spectrum represents.

```{r}
#' Define spectra variables for 3 MS spectra.
sv <- data.frame(
    msLevel = c(2L, 2L, 2L),
    polarity = c(1L, 1L, 1L),
    id = c("HMDB0000001", "HMDB0000001", "HMDB0001847"),
    name = c("1-Methylhistidine", "1-Methylhistidine", "Caffeine"))

#' Assign m/z and intensity values.
sv$mz <- list(
    c(109.2, 124.2, 124.5, 170.16, 170.52),
    c(83.1, 96.12, 97.14, 109.14, 124.08, 125.1, 170.16),
    c(56.0494, 69.0447, 83.0603, 109.0395, 110.0712,
      111.0551, 123.0429, 138.0662, 195.0876))
sv$intensity <- list(
    c(3.407, 47.494, 3.094, 100.0, 13.240),
    c(6.685, 4.381, 3.022, 16.708, 100.0, 4.565, 40.643),
    c(0.459, 2.585, 2.446, 0.508, 8.968, 0.524, 0.974, 100.0, 40.994))

#' Create a Spectra from this data.
s <- Spectra(sv)
s
```

Now we have a `Spectra` object representing 3 mass spectra and a set of spectra
and peak variables.

```{r}
#' List available spectra and peaks variables.
spectraVariables(s)
peaksVariables(s)
```

Spectra are organized linearly, i.e., one after the other. Thus, our `Spectra`
has a length of 3 with each element being one spectrum. Subsetting works as for
any other R object:

```{r}
length(s)

#' Extract the 2nd spectrum
s[2]
```

A `Spectra` *behaves* similar to a `data.frame` with elements (*rows*) being
individual spectra and *columns* being the spectra (or peaks) variables, that
can be accessed with the `$` operator.

```{r}
s$msLevel
```

Similar to a `data.frame` we can also add new spectra variables (columns) using
`$<-`.

```{r}
#' Add a new spectra variable.
s$new_variable <- c("a", "b", "c")
spectraVariables(s)
```

The full peak data can be extracted with the `peaksData` function, that returns
a `list` of two-dimensional arrays with the values of the peak variables. Each
list element represents the peak data from one spectrum, which is stored in a
e.g. `matrix` with columns being the peak variables and rows the respective
values for each peak. The number of rows of such peak variable arrays depends on
the number of mass peaks of each spectrum. This number can be extracted using `lengths`:

```{r}
#' Get the number of peaks per spectrum.
lengths(s)
```

We next extract the peaks' data of our `Spectra` and subset that to data from
the second spectrum.

```{r}
#' Extract the peaks matrix of the second spectrum
peaksData(s)[[2]]
```

Finally, we can also visualize the peak data of a spectrum in the `Spectra`.

```{r}
#' Plot the second spectrum.
plotSpectra(s[2])
```

Such plots could also be created *manually* from the *m/z* and intensity values,
but built-in functions are in most cases more efficient.

```{r}
plot(mz(s)[[2]], intensity(s)[[2]], type = "h",
     xlab = "m/z", ylab = "intensity")
```

Experimental MS data is generally (ideally) stored in files in mzML, mzXML or
CDF format. These are open file formats that can be read by most software. In
Bioconductor, the `r Biocpkg("mzR")` package allows to read/write data in/to
these formats. Also *Spectra* allows to represent data from such raw data
files. To illustrate this we below create a `Spectra` object for from two mzML
files that are provided in Bioconductor's `r Biocpkg("msdata")` package.

```{r}
#' Import MS data from 2 mzML files.
fls <- dir(system.file("sciex", package = "msdata"), full.names = TRUE)
s2 <- Spectra(fls, source = MsBackendMzR())
s2
```

We have thus access to the raw experimental MS data through this `Spectra`
object. In contrast to the first example above, we used here the `Spectra`
constructor providing a `character` vector with the file names and specified
`source = MsBackendMzR()`. This anticipates the concept of
backends in the *Spectra* package: the `source` parameter of the `Spectra` call
defines the backend to use to import (or represent) the data, which is in our
example the `MsBackendMzR` that enables import of data from mzML files.

Like in our first example, we can again subset the object or access any of its
spectra variables. Note however that in this `Spectra` object we have many more
spectra variables available (depending on what is provided by the raw data
files).

```{r}
#' Available spectra variables
spectraVariables(s2)

#' Access the MS levels for the first 6 spectra
s2$msLevel |> head()
```

And we can again visualize the data.

```{r}
#' Plot the 4th spectrum.
plotSpectra(s2[4])
```


## Use of different *backends* with `Spectra`, why and how?

While both `Spectra` objects *behave* the same way, i.e. all data can be
accessed in the same fashion, they actually use different *backends* and thus
data representations:

```{r}
class(s@backend)
class(s2@backend)
```

Our first example `Spectra` uses a `MsBackendMemory` backend that, as the name
tells, keeps all MS data in memory (within the backend object). In contrast, the
second `Spectra` which represents the experimental MS data from the two mzML
files uses the `MsBackendMzR` backend. This backend loads only general spectra
data (variables) into memory while it imports the peak data only *on-demand*
from the original data files when needed/requested (similar to the *on-disk*
mode discussed in [@gattoMSnbaseEfficientElegant2020a]). The advantage of this
*offline* storage mode is a lower memory footprint that enables also the
analysis of large data experiments on *standard* computers.

**Why different backends?**

**Reason 1**: support for additional file formats. The backend class defines the
functionality to import/export data from/to specific file formats. The `Spectra`
class stays *data storage agnostic* and e.g. no import/export routines need to
be implemented for that class. Support for additional file formats can be
implemented independently of the *Spectra* package and can be contributed by
different developers.

Examples:

- `MsBackendMgf` (defined in `r Biocpkg("MsBackendMgf")`) adds support for MS
  files in MGF file format.
- `MsBackendMsp` (defined in `r Biocpkg("MsBackendMsp")` adds support for MSP
  files.
- `MsBackendRawFileReader` (defined in `r Biocpkg("MsBackendRawFileReader")`)
  adds support for MS data files in Thermo Fisher Scientific's raw data file
  format.

To import data from files of a certain file format, the respective backend able
to handle such data needs to be specified with the parameter `source` in the
`Spectra` constructor call:

```{r}
#' Import MS data from mzML files
s_mzr <- Spectra(fls, source = MsBackendMzR())
```

**Reason 2**: support different implementations to store or represent the
data. This includes both *where* the data is stored or *how* it is
stored. Specialized backends can be defined that are, for example, optimized for
high performance or for low memory demand.

Examples:

- `MsBackendMemory` (defined in `r Biocpkg("Spectra")`): keeps all the MS data
  in memory and is optimized for a fast and efficient access to the peaks data
  matrices (i.e., the *m/z* and intensity values of the spectra).
- `MsBackendMzR` (defined in `r Biocpkg("Spectra")`): keeps only spectra
  variables in memory and retrieves the peaks data on-the-fly from the original
  data files upon request. This guarantees a lower memory footprint and enables
  also analysis of large MS experiments.
- `MsBackendSql` (defined in the `r Biocpkg("MsBackendSql")` package): all MS
  data is stored in a SQL database. Has minimal memory requirements but any data
  (whether spectra or peaks variables) need to be retrieved from the
  database. Depending on the SQL database system used, this backend would also
  allow remote data access.

To evaluate and illustrate the properties of these different backends we create
below `Spectra` objects for the same data, but using different backends. We use
the `setBackend` function to change the backend for a `Spectra` object.

```{r}
#' Change the backend to MsBackendMemory
s_mem <- setBackend(s_mzr, MsBackendMemory())
```

With the call above we loaded all MS data from the original data files into
memory. As a third option we next store the full MS data into a SQL database by
using/changing to a `MsBackendOfflineSql` backend defined by the
`r Biocpkg("MsBackendSql")` package.

For the `setBackend` call we need to provide the connection information for the
database that should contain the data. This includes the database driver
(parameter `drv`, depending on the database system), the database name
(parameter `dbname`) as well as eventual additional connection information like
the host, username, port or password. Which of these parameters are required
depends on the SQL database used and hence the driver (see also `?dbConnect` for
more information on the parameters). In our example we will store the data in a
SQLite database. We thus set `drv = SQLite()` and provide with parameter
`dbname` the name of the SQLite database file (that should not yet exist). In
addition, importantly, we disable parallel processing with `BPPARAM =
SerialParam()` since most SQL databases don't support parallel data import.

```{r, echo = FALSE, results = "hide"}
#' Eventually delete an existing SQLite database file
if (file.exists("ms_backend_sql_test.sqlite"))
    file.remove("ms_backend_sql_test.sqlite")
```

```{r}
#' Change the backend to a SQL representation of the data.
library(MsBackendSql)
library(RSQLite)
s_db <- setBackend(s_mzr, MsBackendOfflineSql(), drv = SQLite(),
                   dbname = "ms_backend_sql_test.sqlite",
                   BPPARAM = SerialParam())
```

*Note*: a more efficient way to import MS data from data files into a SQL
database is the `createMsBackendSqlDatabase` from the *MsBackendSql*
package. Also, for larger data sets it is suggested to use more advanced and
powerful SQL database systems (e.g. MySQL/MariaDB SQL databases).

We have now 3 `Spectra` objects representing the same MS data, but using
different backends. As a first comparison we evaluate their size in memory:

```{r}
#' Compare memory footprint.
print(object.size(s_mem), units = "MB")
print(object.size(s_mzr), units = "MB")
print(object.size(s_db), units = "MB")
```

As expected, the size of the `Spectra` object with the `MsBackendMemory` backend
is the largest of the three. Since the `MsBackendOfflineSql` backend keeps only
the primary keys of the spectra in memory it's memory requirements are
particularly small and is thus ideal to represent even very large MS
experiments.

We next evaluate the performance to extract spectra variables. We compare the
time needed to extract the retention times from the 3 `Spectra` objects using
the `microbenchmark` function. With `register(SerialParam())` we globally
disable parallel processing for *Spectra* to ensure the results to be
independent that.

```{r}
#' Compare performance to extract the retention times.
register(SerialParam())
library(microbenchmark)
microbenchmark(
    rtime(s_mem),
    rtime(s_mzr),
    rtime(s_db),
    times = 13
)
```

Highest performance can be seen for the `Spectra` object with the
`MsBackendMemory` backend, followed by the `Spectra` object with the
`MsBackendMzR` backend, while extraction of spectra variables is slowest for the
`Spectra` with the `MsBackendOfflineSql`. Again, this is expected because the
`MsBackendOfflineSql` needs to retrieve the data from the database while the two
other backends keep the retention times of the spectra (along with other general
spectra metadata) in memory.

We next also evaluate the performance to extract peaks data, i.e. the individual
*m/z* and intensity values for all spectra in the two files.

```{r}
#' Compare performance to extract the peaks data.
microbenchmark(
    peaksData(s_mem),
    peaksData(s_mzr),
    peaksData(s_db),
    times = 7
)

```

The `MsBackendMemory` outperforms the two other backends also in this
comparison. Both the `MsBackendMzR` and `MsBackendOfflineSql` need to import
this data, the `MsBackendMzR` from the original mzML files and the
`MsBackendOfflineSql` from the SQL database (which for the present data is more
efficient).

At last we evaluate the performance to subset any of the 3 `Spectra` objects to
10 random spectra.

```{r}
#' Compare performance to subset the Spectra.
idx <- sample(seq_along(s_mem), 10)
microbenchmark(
    s_mem[idx],
    s_mzr[idx],
    s_db[idx]
)
```

Here, the `MsBackendOfflineSql` has a clear advantage over the two other
backends, because it only needs to subset the integer vector of primary keys
(the only data it keeps in memory), while the `MsBackendMemory` needs to subset
the full data, and the `MsBackendMzR` the data frame with the spectra variables.


### Performance considerations and tweaks

Keeping all data in memory (e.g. by using a `MsBackendMemory`) has its obvious
performance advantages, but might not be possible for all MS
experiments. *On-disk* backends such as the `MsBackendMzR` or
`MsBackendOfflineSql` allow, due to their lower memory footprint, to analyze
also very large data sets. This is further optimized by the build-in option for
most `Spectra` methods to load and process MS data only in small chunks in
contrast to loading and processing the full data as a whole. The performance of
the *offline* backends `MsBackendMzR` and `MsBackendSql`/`MsBackendOfflineSql`
depend also on the I/O capabilities of the hard disks containing the data files
and, for the latter, in addition on the configuration of the SQL database system
used.



## Data manipulation and the lazy evaluation queue

Not all data representations might, by design or because of the way the data is
handled and stored, allow changing existing or adding new values. The
`MsBackendMzR` backend for example retrieves the peaks data directly from the
raw data files and we don't want any manipulation of *m/z* or intensity values
to be directly written back to these original data files. Also other backends,
specifically those that provide access to spectral reference databases (such as
the `MsBackendMassbankSql` from the `r Biocpkg("MsBackendMassbank")` package or
the `MsBackendCompDb` from the `r Biocpkg("CompoundDb")` package), are not
supposed to allow any changes to the information stored in these databases.

Data manipulations are however an integral part of any data analysis and we thus
need for such backends some mechanism that allows changes to data without
propagating these to the original data resource. For some backend classes a
*caching* mechanism is implemented that enables adding, changing, or deleting
spectra variables. To illustrate this we below add a new spectra variable
`"new_variable"` to each of our 3 `Spectra` objects.

```{r}
#' Assign new spectra variables.
s_mem$new_variable <- seq_along(s_mem)
s_mzr$new_variable <- seq_along(s_mem)
s_db$new_variable <- seq_along(s_mem)
```

A new spectra variable was now added to each of the 3 `Spectra` objects:

```{r}
#' Show spectra variables for one of the Spectra
spectraVariables(s_mzr)
```

The `MsBackendMemory` stores all spectra variables in a `data.frame` within the
object. The new spectra variable was thus simply added as a new column to this
`data.frame`.

**Warning**: **NEVER** access or manipulate any of the slots of a backend class
directly like below as this can easily result in data corruption.

```{r}
#' Direct access to the backend's data.
s_mem@backend@spectraData |> head()
```

Similarly, also the `MsBackendMzR` stores data for spectra variables (but not
peaks variables!) within a `DataFrame` inside the object. Adding a new spectra
variable thus also added a new column to this `DataFrame`. *Note*: the use of a
`DataFrame` instead of a `data.frame` in `MsBackendMzR` explains most of the
performance differences to subset or access spectra variables we've seen between
this backend and the `MsBackendMemory` above.

```{r}
s_mzr@backend@spectraData
```

The `MsBackendOfflineSql` implements the support for changes to spectra
variables differently: the object contains a vector with the available database
column names and in addition an (initially empty) `data.frame` to cache changes
to spectra variables. If a spectra variable is deleted, only the respective
column name is removed from the character vector with available column names. If
a new spectra variable is added (like in the example above) or if values of an
existing spectra variable are changed, this spectra variable, respectively its
values, are stored (as a new column) in the caching data frame:

```{r}
s_db@backend@localData |> head()
```

Actually, why do we not want to change the data directly in the database? It
should be fairly simple to add a new column to the database table or change the
values in that. There are actually two reasons to **not** do that: firstly we
don't want to change the *raw* data that might be stored in the database
(e.g. if the database contains reference MS2 spectra, such as in the case of a
`Spectra` with MassBank data) and secondly, we want to avoid the following
situation: imagine we create a copy of our `s_db` variable and change the
retention times in that variable:

```{r}
#' Change values in a copy of the variable
s_db2 <- s_db
rtime(s_db2) <- rtime(s_db2) + 10
```

Writing these changes back to the database would also change the retention time
values in the original `s_db` variable and that is for obvious reasons not
desired.

```{r}
#' Retention times of original copy should NOT change
rtime(s_db) |> head()
rtime(s_db2) |> head()
```

Note that this can also be used for `MsBackendOfflineSql` backends to
cache spectra variables in memory for faster access.

```{r}
s_db2 <- s_db

#' Cache an existing spectra variable in memory
s_db2$rtime <- rtime(s_db2)

microbenchmark(
    rtime(s_db),
    rtime(s_db2),
    times = 17
)
```

*Note*: the `MsBackendCached` class from the *Spectra* package provides the
internal `data.frame`-based caching mechanism described above. The
`MsBackendOfflineSql` class directly extends this backend and hence inherits
this mechanism and only database-specific additional functionality needed to be
implemented. Thus, any `MsBackend` implementation extending the
`MsBackendCached` base class automatically inherit this caching mechanism and do
not need to implement their own. Examples of backends extending
`MsBackendCached` are, among others, `MsBackendSql`, `MsBackendOfflineSql`, and
`MsBackendMassbankSql`.

Analysis of MS data requires, in addition to changing or adding spectra
variables also the possibility to manipulate the peaks' data (i.e. the *m/z* and
intensity values of the individual mass peaks of the spectra). As a simple
example we remove below all mass peaks with an intensity below 100 from the data
set and compare the number of peaks per spectra before and after that operation.

```{r}
#' Remove all peaks with an intensity below 100
s_mem <- filterIntensity(s_mem, intensity = 100)

#' Compare the number of peaks before/after
boxplot(list(before = lengths(s_mzr), after = lengths(s_mem)),
        ylab = "number of peaks")
```

This operation did reduce the number of peaks considerably. We repeat this
operation now also for the two other backends.

```{r}
#' Repeast filtering for the two other Spectra objects.
s_mzr <- filterIntensity(s_mzr, intensity = 100)
s_db <- filterIntensity(s_db, intensity = 100)
```

The number of peaks were also reduced for these two backends although they do
not keep the peak data in memory and, as discussed before, we do not allow
changes to the data file (or the database) containing the original data.

```{r}
#' Evaluate that subsetting worked for all.
median(lengths(s_mem))
median(lengths(s_mzr))
median(lengths(s_db))
```

The caching mechanisms described above work well for spectra variables because
of their (generally) limited and manageable sizes. MS peaks data (*m/z* and
intensity values of the individual peaks of MS spectra) however represent much
larger data volumes preventing to cache and store changes to these directly
in-memory, especially for data from large experiments or high resolution
instruments.

For that reason, instead of caching changes to the values within the object, we
cache the actual data manipulation operation(s). The function to modify peaks
data, along with all of its parameters, is automatically added to an internal
*lazy processing queue* within the `Spectra` object for each call to one of the
`Spectra`'s (peaks) data manipulation functions. This mechanism is implemented
directly for the `Spectra` class and backends thus do not have to implement
their own.

When calling `filterIntensity` above, the data was actually not modified, but
the function to filter the peaks data was added to this processig queue. The
function along with all possibly defined parameters was added as a
`ProcessingStep` object:

```{r}
#' Show the processing queue.
s_db@processingQueue
```

Again, it is not advisable to directly access any of the internal slots of a
`Spectra` object! The function could be accessed with:

```{r}
#' Access a processing step's function
s_db@processingQueue[[1L]]@FUN
```

and all of its parameters with:

```{r}
#' Access the parameters for that function
s_db@processingQueue[[1L]]@ARGS
```

Each time peaks data is accessed (like with the `intensity` call in the example
below), the `Spectra` object will first request the *raw* peaks data from the
backend, check its own processing queue and, if that is not empty, apply each of
the contained cached processing steps to the peaks data before returning it to
the user.

```{r}
#' Access intensity values and extract those of the 1st spectrum.
intensity(s_db)[[1L]]
```

As a nice side effect, if only a subset of the data is accessed, the data
manipulations need only to be applied to the requested data subset, and not the
full data. This can have a positive impact on overall performance:

```{r}
#' Compare applying the processing queue to either 10 spectra or the
#' full set of spectra.
microbenchmark(
    intensity(s_mem[1:10]),
    intensity(s_mem)[1:10],
    times = 7)
```

Next to the number of common peaks data manipulation methods that are already
implemented for `Spectra`, and that all make use of this processing queue, there
is also the `addProcessing` function that allows to apply any user-provided
function to the peaks data (using the same lazy evaluation mechanism). As a
simple example we define below a function that *scales* all intensities in a
spectrum such that the total intensity sum per spectrum is 1. Functions for
`addProcessing` are expected to take a peaks array as input and should again
return a peaks array as their result. Note also that the `...` in the function
definition below is required, because internally additional parameters, such as
the spectrum's MS level, are by default passed along to the function (see also
the `addProcessing` documentation entry in `?Spectra` for more information and
more advanced examples).

```{r}
#' Define a function that scales intensities
scale_sum <- function(x, ...) {
    x[, "intensity"] <- x[, "intensity"] / sum(x[, "intensity"], na.rm = TRUE)
    x
}

#' Add this function to the processing queue
s_mem <- addProcessing(s_mem, scale_sum)
s_mzr <- addProcessing(s_mzr, scale_sum)
s_db <- addProcessing(s_db, scale_sum)
```

As a validation we calculate the sum of intensities for the first 6 spectra.

```{r}
sum(intensity(s_mem)) |> head()
sum(intensity(s_mzr)) |> head()
sum(intensity(s_db)) |> head()
```

All intensities were thus scaled to a total intensity sum per spectra of 1.
Note that, while this is a common operation for MS2 data (fragment spectra),
such a scaling function should generally not be used for (quantitative) MS1 data
(as in our example here).

Finally, since through this lazy evaluation mechanism we are not changing actual
peaks data, we can also *undo* data manipulations. A simple `reset` call on a
`Spectra` object will restore the data in the object to its initial state (in
fact it simply clears the processing queue).

```{r}
#' Restore the Spectra object to its initial state.
s_db <- reset(s_db)
median(lengths(s_db))
sum(intensity(s_db)) |> head()
```

Along with potential chaching mechanisms for spectra variables, this
implementation of a lazy-evaluation queue allows to apply also data
manipulations to MS data from data resources that would be inherently
*read-only* (such as reference libraries of fragment spectra) and thus enable
the analysis of MS data independently of *where* and *how* the data is stored.


## Implementing your own `MsBackend`

- New backends for `Spectra` should extend the `MsBackend` class and implement
  the methods defined by its infrastructure.
- Extending other classes, such as the `MsBackendMemory` or `MsBackendCached`
  can help reducing the number of methods or concepts needed to implement.
- A detailed example and description is provided in the [*Creating new
  `MsBackend` classes*](https://rformassspectrometry.github.io/Spectra....)
  vignette - otherwise: open an issue on the [*Spectra* github
  repo](https://github.com/RforMassSpectrometry/Spectra).
- Add the following code to your *testthat.R* file to ensure compliance with the
  `MsBackend` definition:
  ```
  test_suite <- system.file("test_backends", "test_MsBackend",
                          package = "Spectra")

  #' Assign an instance of the developed `MsBackend` to variable `be`.
  be <- my_be
  test_dir(test_suite, stop_on_failure = TRUE)
  ```

## Final words

- The `r Biocpkg("Spectra")` package provides a powerful infrastructure to
  handle and process MS data in R:
  - designed to support very large data sets.
  - easily extendable.
  - support for a variety of MS data file formats and data representations.
  - caching mechanism and processing queue allow data analysis also of
    *read-only* data resources.



# Acknowledgments

Thank you to [Philippine Louail](https://github.com/philouail) for fixing typos
and suggesting improvements.

# References
